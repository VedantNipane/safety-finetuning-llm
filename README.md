# RSAI project to test the safety with finetuning LLMs
### Implementation of the paper : Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!
### The implementation is referred from this repository by fixing few run time issues: https://github.com/LLM-Tuning-Safety/LLMs-Finetuning-Safety
### The dataset is hugging face gated dataset and access should be approved by authors. Access request to be raised from https://huggingface.co/datasets/LLM-Tuning-Safety/HEx-PHI
# RSAI project to test the safety with finetuning LLMs
### Implementation of the paper : Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!
### The implementation is refered from this repository by fixing few run time issues: https://github.com/LLM-Tuning-Safety/LLMs-Finetuning-Safety